{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9f94630-361b-4be0-9a2c-ff43537163a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def download_pdfs(url, download_folder):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all rows in the table of litigation releases\n",
    "        rows = soup.find_all('tr', class_='pr-list-page-row')\n",
    "\n",
    "        for row in rows:\n",
    "            date_field = row.find('td', class_='views-field views-field-field-publish-date')\n",
    "            respondent_field = row.find('td', class_='views-field views-field-field-release-file-number views-field-nothing-1')\n",
    "            if date_field and respondent_field:\n",
    "                date = date_field.time.text if date_field.time else 'No_Date'\n",
    "                respondents = respondent_field.div.text.strip().replace(' ', '_').replace('/', '_') if respondent_field.div else 'No_Respondents'\n",
    "                pdf_link = respondent_field.find('a', {'type': 'application/pdf'})\n",
    "\n",
    "                if pdf_link:\n",
    "                    pdf_url = 'https://www.sec.gov' + pdf_link['href']\n",
    "                    pdf_name = f\"{date}_{respondents}.pdf\"\n",
    "\n",
    "                    print(f\"Downloading: {pdf_name} from {pdf_url}\")\n",
    "                    pdf_response = requests.get(pdf_url)\n",
    "\n",
    "                    if not os.path.exists(download_folder):\n",
    "                        os.makedirs(download_folder)\n",
    "\n",
    "                    with open(os.path.join(download_folder, pdf_name), 'wb') as f:\n",
    "                        f.write(pdf_response.content)\n",
    "                        print(f'Downloaded: {pdf_name}')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "sec_url = 'https://www.sec.gov/litigation/litreleases'\n",
    "download_folder = '/Users/Ray/Desktop/SEC_PDFs'\n",
    "\n",
    "download_pdfs(sec_url, download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe382b4-ebfd-4566-9cf2-7267657c7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently working scrapping file \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "def download_pdfs_from_page(url, session, download_folder):\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully connected to the page.\")\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        else:\n",
    "            print(f\"Failed to connect, status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        rows = soup.find_all('tr', class_='pr-list-page-row')\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                date_field = row.find('td', class_='views-field views-field-field-publish-date')\n",
    "                respondent_field = row.find('td', class_='views-field views-field-field-release-file-number views-field-nothing-1')\n",
    "                if date_field and respondent_field:\n",
    "                    date = date_field.time.text.strip().replace(' ', '_').replace('.', '') if date_field.time else 'No_Date'\n",
    "                    respondents = respondent_field.div.text.strip().replace(' ', '_').replace('/', '_') if respondent_field.div else 'No_Respondents'\n",
    "                    pdf_link = respondent_field.find('a', {'type': 'application/pdf'})\n",
    "\n",
    "                    if pdf_link:\n",
    "                        pdf_url = 'https://www.sec.gov' + pdf_link['href']\n",
    "                        document_type = pdf_link.text.strip().split(' ')[0].lower()\n",
    "                        pdf_name = f\"{date}_{respondents}_{document_type}.pdf\"\n",
    "\n",
    "                        # Truncate pdf_name to a maximum length (e.g., 255 characters)\n",
    "                        max_length = 255\n",
    "                        if len(pdf_name) > max_length:\n",
    "                            pdf_name = pdf_name[:max_length - 4] + '.pdf'\n",
    "\n",
    "                        print(f\"Downloading: {pdf_name} from {pdf_url}\")\n",
    "                        pdf_response = session.get(pdf_url)\n",
    "\n",
    "                        if not os.path.exists(download_folder):\n",
    "                            os.makedirs(download_folder)\n",
    "\n",
    "                        with open(os.path.join(download_folder, pdf_name), 'wb') as f:\n",
    "                            f.write(pdf_response.content)\n",
    "                            print(f'Downloaded: {pdf_name}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while downloading the PDF: {e}\")\n",
    "                continue  # Continue to the next PDF even if the current one fails\n",
    "\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_next_page_url(current_soup, base_url):\n",
    "    if current_soup:\n",
    "        next_page_link = current_soup.find('a', {'title': 'Go to next page'})\n",
    "        if next_page_link:\n",
    "            next_page_url = base_url + next_page_link['href']\n",
    "            print(f\"Next page URL: {next_page_url}\")\n",
    "            return next_page_url\n",
    "    return None\n",
    "\n",
    "def download_all_pdfs(base_url, download_folder):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': base_url\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "        current_url = base_url\n",
    "        while current_url:\n",
    "            print(f\"Processing page: {current_url}\")\n",
    "            current_soup = download_pdfs_from_page(current_url, session, download_folder)\n",
    "            if current_soup is None:\n",
    "                print(\"Failed to download from the current page, stopping the script.\")\n",
    "                break\n",
    "            time.sleep(5)  # Delay to avoid rate limiting\n",
    "            current_url = get_next_page_url(current_soup, base_url)\n",
    "\n",
    "download_folder = '/Users/Ray/Desktop/SECLITIgation'\n",
    "base_url = 'https://www.sec.gov/litigation/litreleases'\n",
    "\n",
    "download_all_pdfs(base_url, download_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6111b5bd-dd62-4997-a6bf-482def7ca1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path where the Excel files are located\n",
    "path = '/Users/Ray/Desktop/'\n",
    "\n",
    "# Load the two Excel files\n",
    "df1 = pd.read_excel(path + '1.xlsx')\n",
    "df2 = pd.read_excel(path + '2.xlsx')\n",
    "\n",
    "# Merge the two dataframes based on the 'Respondents' column\n",
    "combined_df = pd.merge(df1, df2, on='Respondents', how='inner')\n",
    "\n",
    "# Save the combined dataframe to a new Excel file on the desktop\n",
    "combined_df.to_excel(path + 'combined.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42131d4d-d0a1-47a4-98a4-7d2282050e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path where the Excel file is located\n",
    "path = '/Users/RayXu/Desktop/'\n",
    "\n",
    "# Assuming 'combined.xlsx' is the file you've already created and contains the columns as you mentioned\n",
    "combined_df = pd.read_excel(path + 'SECLitigation.xlsx')\n",
    "\n",
    "# Reorder the columns to put 'Date' first\n",
    "columns_order = ['Date', 'Respondents', 'Release No','Case Number', 'Filename', 'Document Type','PDF  Link']\n",
    "combined_df = combined_df[columns_order]\n",
    "\n",
    "# Save the reordered dataframe to a new Excel file\n",
    "combined_df.to_excel(path + 'reordered_combined.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b20e85-516b-4b76-85b6-e61d226ae97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
